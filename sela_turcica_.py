# -*- coding: utf-8 -*-
"""Sela Túrcica - com IC95%

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J90wEPPQrCjwXwpZP-S7DuBDMyi1yk9y

# **Library and packages**
"""

import pandas as pd
import warnings
warnings.filterwarnings("ignore")
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict, KFold
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import numpy as np
from sklearn import metrics
from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import roc_curve, auc

"""# **Dataset**"""

file_path = '/content/dataset4.xlsx'
df = pd.read_excel(file_path)

df.columns

"""# **Data preprocessing**"""

colunas = ['sex','NS=nasium-sella','PS=ponticulus-sela']

df = df.loc[:, colunas].dropna()

coluna_grupo = df.pop('sex')
df.insert(0, 'sex', coluna_grupo)

df.info()
df.columns

contagem_sex = df['sex'].value_counts()
print(contagem_sex)

"""# **Model building**

**Data splitting**
"""

X = df.iloc[:,1:]
y = df.iloc[:,0]

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split


RANDOM_STATE = 42
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state=RANDOM_STATE, shuffle=True)

#SMOTE - oversampling (only train data)
oversample = SMOTE(random_state=RANDOM_STATE)
X_train, y_train = oversample.fit_resample(X_train, y_train)


print(f"Train data shape of X = {X_train.shape} and Y = {y_train.shape}")
print(f"Test data shape of X = {X_test.shape} and Y = {y_test.shape}")

y_test.value_counts()

##Data normalization
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

y_train = y_train.astype('category')
y_test = y_test.astype('category')

# K-Folds
fold = 5

"""**GRADIENT BOOSTING**"""

import numpy as np
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV, KFold, cross_val_predict, train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_score, recall_score, f1_score
from sklearn.utils import resample

param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

gb_model = GradientBoostingClassifier(random_state=RANDOM_STATE)
grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
best_gb_model = grid_search.best_estimator_
best_gb_model.fit(X_train, y_train)
y_pred_test_gb = best_gb_model.predict(X_test)
y_prob_test_gb = best_gb_model.predict_proba(X_test)[:, 1]

def bootstrap_metrics(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    metrics = {'accuracy': [], 'precision_weighted': [], 'recall_weighted': [], 'f1_weighted': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_pred = model.predict(X_resampled)
        metrics['accuracy'].append(accuracy_score(y_resampled, y_pred))
        metrics['precision_weighted'].append(precision_score(y_resampled, y_pred, average='weighted', zero_division=0))
        metrics['recall_weighted'].append(recall_score(y_resampled, y_pred, average='weighted'))
        metrics['f1_weighted'].append(f1_score(y_resampled, y_pred, average='weighted'))
    return metrics

metrics_test = bootstrap_metrics(best_gb_model, X_test, y_test)

def ci95(data):
    return np.percentile(data, [2.5, 97.5])

accuracy_ci_test = ci95(metrics_test['accuracy'])
precision_ci_test = ci95(metrics_test['precision_weighted'])
recall_ci_test = ci95(metrics_test['recall_weighted'])
f1_ci_test = ci95(metrics_test['f1_weighted'])

print(f"Test Data Accuracy: {np.mean(metrics_test['accuracy'])} (95% CI: {accuracy_ci_test})")
print(f"Test Data Precision (Weighted): {np.mean(metrics_test['precision_weighted'])} (95% CI: {precision_ci_test})")
print(f"Test Data Recall (Weighted): {np.mean(metrics_test['recall_weighted'])} (95% CI: {recall_ci_test})")
print(f"Test Data F1 Score (Weighted): {np.mean(metrics_test['f1_weighted'])} (95% CI: {f1_ci_test})")

conf_matrix_gb = confusion_matrix(y_test, y_pred_test_gb)
accuracy_gb = accuracy_score(y_test, y_pred_test_gb)
report_gb = classification_report(y_test, y_pred_test_gb, zero_division=0)

print("Confusion Matrix (Test Data):\n", conf_matrix_gb)
print("Accuracy:", accuracy_gb)
print("Classification Report (Test Data):\n", report_gb)

kf_gb = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_prob_cv_gb = cross_val_predict(best_gb_model, X_train, y_train, cv=kf_gb, method='predict_proba')[:, 1]
y_pred_cv_gb = cross_val_predict(best_gb_model, X_train, y_train, cv=kf_gb)

accuracy_cv_scores = []
precision_cv_scores = []
recall_cv_scores = []
f1_cv_scores = []

for train_index, test_index in kf_gb.split(X_train):
    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]
    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]
    best_gb_model.fit(X_train_fold, y_train_fold)
    y_pred_fold = best_gb_model.predict(X_test_fold)
    accuracy_cv_scores.append(accuracy_score(y_test_fold, y_pred_fold))
    precision_cv_scores.append(precision_score(y_test_fold, y_pred_fold, average='weighted', zero_division=0))
    recall_cv_scores.append(recall_score(y_test_fold, y_pred_fold, average='weighted'))
    f1_cv_scores.append(f1_score(y_test_fold, y_pred_fold, average='weighted'))

accuracy_ci_cv = ci95(accuracy_cv_scores)
precision_ci_cv = ci95(precision_cv_scores)
recall_ci_cv = ci95(recall_cv_scores)
f1_ci_cv = ci95(f1_cv_scores)

print(f"Cross-Validation Accuracy: {np.mean(accuracy_cv_scores)} (95% CI: {accuracy_ci_cv})")
print(f"Cross-Validation Precision (Weighted): {np.mean(precision_cv_scores)} (95% CI: {precision_ci_cv})")
print(f"Cross-Validation Recall (Weighted): {np.mean(recall_cv_scores)} (95% CI: {recall_ci_cv})")
print(f"Cross-Validation F1 Score (Weighted): {np.mean(f1_cv_scores)} (95% CI: {f1_ci_cv})")

conf_matrix_cv_gb = confusion_matrix(y_train, y_pred_cv_gb)
accuracy_cv_gb = accuracy_score(y_train, y_pred_cv_gb)
report_cv_gb = classification_report(y_train, y_pred_cv_gb, zero_division=0)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv_gb)
print("Cross-Validation Accuracy:", accuracy_cv_gb)
print("Classification Report (Cross-Validation):\n", report_cv_gb)

fpr_gb, tpr_gb, _ = roc_curve(y_test, y_prob_test_gb, pos_label=2)
roc_auc_gb = auc(fpr_gb, tpr_gb)
fpr_cv_gb, tpr_cv_gb, _ = roc_curve(y_train, y_prob_cv_gb, pos_label=2)
roc_auc_cv_gb = auc(fpr_cv_gb, tpr_cv_gb)


def bootstrap_auc(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    auc_scores = []
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_prob = model.predict_proba(X_resampled)[:, 1]
        fpr, tpr, _ = roc_curve(y_resampled, y_prob, pos_label=2)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

fpr_test, tpr_test, _ = roc_curve(y_test, y_prob_test_gb, pos_label=2)
auc_test = auc(fpr_test, tpr_test)
auc_scores_test_gb = bootstrap_auc(best_gb_model, X_test, y_test)

auc_ci_test_gb = np.percentile(auc_scores_test_gb, [2.5, 97.5])

print(f"Test Data AUC: {auc_test} (95% CI: {auc_ci_test_gb})")


def auc_cross_val(model, X, y, cv, pos_label=2):
    auc_scores = []
    for train_index, test_index in cv.split(X):
        model.fit(X[train_index], y[train_index])
        y_prob = model.predict_proba(X[test_index])[:, 1]
        fpr, tpr, _ = roc_curve(y[test_index], y_prob, pos_label=2)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

auc_scores_cv_gb = auc_cross_val(best_gb_model, X_train, y_train, kf_gb, pos_label=2)

def ci95(data):
    return np.percentile(data, [2.5, 97.5])

auc_ci_cv_gb = ci95(auc_scores_cv_gb)

print(f"Cross-Validation AUC: {np.mean(auc_scores_cv_gb)} (95% CI: {auc_ci_cv_gb})")

###Calculation of ROC Curve metrics and graph plotting (Test data and cross-validation)
fpr_gb, tpr_gb, _ = roc_curve(y_test, y_prob_test_gb, pos_label = 2)
roc_auc_gb = auc(fpr_gb, tpr_gb)

plt.figure(figsize=(8, 8))
plt.plot(fpr_gb, tpr_gb, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_gb))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Test Data')
plt.legend(loc='lower right')
plt.show()

# Cross-validation data
fpr_cv_gb, tpr_cv_gb, _ = roc_curve(y_train, y_prob_cv_gb, pos_label = 2)
roc_auc_cv_gb = auc(fpr_cv_gb, tpr_cv_gb)

plt.figure(figsize=(8, 8))
plt.plot(fpr_cv_gb, tpr_cv_gb, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_cv_gb))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Cross-Validation')
plt.legend(loc='lower right')
plt.show()

##FEATURE IMPORTANCE GRADIENT BOOSTING CLASSIFIER
X = df.drop('sex', axis=1)
features=[]
for columns in X.columns:
    features.append(columns)
imp_features = best_gb_model.feature_importances_
importances = best_gb_model.feature_importances_

feature_importance_gb = pd.DataFrame({'Feature': features, 'Importance': importances})

feature_importance_sorted = feature_importance_gb.sort_values('Importance', ascending=True)
colors = plt.cm.magma(np.linspace(0.2, 1, len(feature_importance_sorted)))
plt.figure(figsize=(10, 6))
bars = plt.barh(feature_importance_sorted['Feature'], feature_importance_sorted['Importance'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')

for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')

plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()
plt.show()

plt.savefig('figimpKNN.jpg', dpi=300, bbox_inches='tight')

"""**LOGISTIC REGRESSION**"""

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV, KFold, cross_val_predict, train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_score, recall_score, f1_score, roc_curve, auc
from sklearn.utils import resample

param_grid = {
    'C': [0.001, 0.005, 0.01, 0.02, 0.03, 0.05, 0.1, 0.2, 0.3, 0.5, 1, 5, 10, 100],
    'penalty': ['l1', 'l2', 'elasticnet', 'none'],
    'max_iter': [50, 100, 300, 500, 1000],
    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],
    'l1_ratio': [0.2, 0.4, 0.6, 0.8]
}

logreg_model = LogisticRegression(random_state=RANDOM_STATE)
grid_search = GridSearchCV(estimator=logreg_model, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
best_logreg_model = grid_search.best_estimator_
best_logreg_model.fit(X_train, y_train)
y_pred_test_logreg = best_logreg_model.predict(X_test)
y_prob_test_logreg = best_logreg_model.predict_proba(X_test)[:, 1]

def bootstrap_metrics(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    metrics = {'accuracy': [], 'precision_weighted': [], 'recall_weighted': [], 'f1_weighted': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_pred = model.predict(X_resampled)
        metrics['accuracy'].append(accuracy_score(y_resampled, y_pred))
        metrics['precision_weighted'].append(precision_score(y_resampled, y_pred, average='weighted', zero_division=0))
        metrics['recall_weighted'].append(recall_score(y_resampled, y_pred, average='weighted'))
        metrics['f1_weighted'].append(f1_score(y_resampled, y_pred, average='weighted'))
    return metrics

metrics_test = bootstrap_metrics(best_logreg_model, X_test, y_test)

def ci95(data):
    return np.percentile(data, [2.5, 97.5])

accuracy_ci_test = ci95(metrics_test['accuracy'])
precision_ci_test = ci95(metrics_test['precision_weighted'])
recall_ci_test = ci95(metrics_test['recall_weighted'])
f1_ci_test = ci95(metrics_test['f1_weighted'])

print(f"Test Data Accuracy: {np.mean(metrics_test['accuracy'])} (95% CI: {accuracy_ci_test})")
print(f"Test Data Precision (Weighted): {np.mean(metrics_test['precision_weighted'])} (95% CI: {precision_ci_test})")
print(f"Test Data Recall (Weighted): {np.mean(metrics_test['recall_weighted'])} (95% CI: {recall_ci_test})")
print(f"Test Data F1 Score (Weighted): {np.mean(metrics_test['f1_weighted'])} (95% CI: {f1_ci_test})")

conf_matrix_logreg = confusion_matrix(y_test, y_pred_test_logreg)
accuracy_logreg = accuracy_score(y_test, y_pred_test_logreg)
report_logreg = classification_report(y_test, y_pred_test_logreg, zero_division=0)

print("Confusion Matrix (Test Data):\n", conf_matrix_logreg)
print("Accuracy:", accuracy_logreg)
print("Classification Report (Test Data):\n", report_logreg)

kf_logreg = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_prob_cv_logreg = cross_val_predict(best_logreg_model, X_train, y_train, cv=kf_logreg, method='predict_proba')[:, 1]
y_pred_cv_logreg = cross_val_predict(best_logreg_model, X_train, y_train, cv=kf_logreg)

accuracy_cv_scores = []
precision_cv_scores = []
recall_cv_scores = []
f1_cv_scores = []

for train_index, test_index in kf_logreg.split(X_train):
    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]
    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]
    best_logreg_model.fit(X_train_fold, y_train_fold)
    y_pred_fold = best_logreg_model.predict(X_test_fold)
    accuracy_cv_scores.append(accuracy_score(y_test_fold, y_pred_fold))
    precision_cv_scores.append(precision_score(y_test_fold, y_pred_fold, average='weighted', zero_division=0))
    recall_cv_scores.append(recall_score(y_test_fold, y_pred_fold, average='weighted'))
    f1_cv_scores.append(f1_score(y_test_fold, y_pred_fold, average='weighted'))

accuracy_ci_cv = ci95(accuracy_cv_scores)
precision_ci_cv = ci95(precision_cv_scores)
recall_ci_cv = ci95(recall_cv_scores)
f1_ci_cv = ci95(f1_cv_scores)

print(f"Cross-Validation Accuracy: {np.mean(accuracy_cv_scores)} (95% CI: {accuracy_ci_cv})")
print(f"Cross-Validation Precision (Weighted): {np.mean(precision_cv_scores)} (95% CI: {precision_ci_cv})")
print(f"Cross-Validation Recall (Weighted): {np.mean(recall_cv_scores)} (95% CI: {recall_ci_cv})")
print(f"Cross-Validation F1 Score (Weighted): {np.mean(f1_cv_scores)} (95% CI: {f1_ci_cv})")

conf_matrix_cv_logreg = confusion_matrix(y_train, y_pred_cv_logreg)
accuracy_cv_logreg = accuracy_score(y_train, y_pred_cv_logreg)
report_cv_logreg = classification_report(y_train, y_pred_cv_logreg, zero_division=0)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv_logreg)
print("Cross-Validation Accuracy:", accuracy_cv_logreg)
print("Classification Report (Cross-Validation):\n", report_cv_logreg)

fpr_logreg, tpr_logreg, _ = roc_curve(y_test, y_prob_test_logreg, pos_label=2)
roc_auc_logreg = auc(fpr_logreg, tpr_logreg)
fpr_cv_logreg, tpr_cv_logreg, _ = roc_curve(y_train, y_prob_cv_logreg, pos_label=2)
roc_auc_cv_logreg = auc(fpr_cv_logreg, tpr_cv_logreg)

def bootstrap_auc(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    auc_scores = []
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_prob = model.predict_proba(X_resampled)[:, 1]
        fpr, tpr, _ = roc_curve(y_resampled, y_prob, pos_label=2)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

fpr_test, tpr_test, _ = roc_curve(y_test, y_prob_test_logreg, pos_label=2)
auc_test_lr = auc(fpr_test, tpr_test)
auc_scores_test_lr = bootstrap_auc(best_logreg_model, X_test, y_test)

auc_ci_test_lr = np.percentile(auc_scores_test_lr, [2.5, 97.5])

print(f"Test Data AUC: {auc_test_lr} (95% CI: {auc_ci_test_lr})")

def auc_cross_val(model, X, y, cv, pos_label=2):
    auc_scores = []
    for train_index, test_index in cv.split(X):
        model.fit(X[train_index], y[train_index])
        y_prob = model.predict_proba(X[test_index])[:, 1]
        fpr, tpr, _ = roc_curve(y[test_index], y_prob, pos_label=2)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

auc_scores_cv_lr = auc_cross_val(best_logreg_model, X_train, y_train, kf_logreg, pos_label=2)

def ci95(data):
    return np.percentile(data, [2.5, 97.5])

auc_ci_cv_lr = ci95(auc_scores_cv_lr)

print(f"Cross-Validation AUC: {np.mean(auc_scores_cv_lr)} (95% CI: {auc_ci_cv_lr})")

#Calculation of ROC Curve metrics and graph plotting
fpr_logreg, tpr_logreg, _ = roc_curve(y_test, y_prob_test_logreg, pos_label = 2)
roc_auc_logreg = auc(fpr_logreg, tpr_logreg)

plt.figure(figsize=(8, 8))
plt.plot(fpr_logreg, tpr_logreg, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_logreg))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Test Data')
plt.legend(loc='lower right')
plt.show()
fpr_cv_logreg, tpr_cv_logreg, _ = roc_curve(y_train, y_prob_cv_logreg, pos_label = 2)
roc_auc_cv_logreg = auc(fpr_cv_logreg, tpr_cv_logreg)
plt.figure(figsize=(8, 8))
plt.plot(fpr_cv_logreg, tpr_cv_logreg, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_cv_logreg))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Cross-Validation')
plt.legend(loc='lower right')
plt.show()

#FEATURE IMPORTANCE LOGISTIC REGRESSION
model = best_logreg_model.fit(X_train, y_train)
coefficients = model.coef_[0]
feature_importance_lr = pd.DataFrame({'Feature': X.columns, 'Importance': np.abs(coefficients)})
feature_importance_lr = feature_importance_lr.sort_values('Importance', ascending=True)
colors = plt.cm.plasma(np.linspace(0.2, 1, len(feature_importance_lr)))
plt.figure(figsize=(10, 6))
bars = plt.barh(feature_importance_lr['Feature'], feature_importance_lr['Importance'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')
for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()

plt.show()

"""# SVM"""

param_grid = {
    'C': [0.001, 0.1, 0.9, 1.5, 2.3, 23, 50, 100],
    'kernel': ['linear', 'rbf'],
    'gamma': ['auto', 'scale']
}

svc_model = SVC(random_state=RANDOM_STATE, probability=True)
grid_search = GridSearchCV(estimator=svc_model, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
best_svc_model = grid_search.best_estimator_
best_svc_model.fit(X_train, y_train)
y_pred_test_svc = best_svc_model.predict(X_test)
y_prob_test_svc = best_svc_model.predict_proba(X_test)[:, 1]

def bootstrap_metrics(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    metrics = {'accuracy': [], 'precision_weighted': [], 'recall_weighted': [], 'f1_weighted': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_pred = model.predict(X_resampled)
        metrics['accuracy'].append(accuracy_score(y_resampled, y_pred))
        metrics['precision_weighted'].append(precision_score(y_resampled, y_pred, average='weighted', zero_division=0))
        metrics['recall_weighted'].append(recall_score(y_resampled, y_pred, average='weighted'))
        metrics['f1_weighted'].append(f1_score(y_resampled, y_pred, average='weighted'))
    return metrics

metrics_test = bootstrap_metrics(best_svc_model, X_test, y_test)

def ci95(data):
    return np.percentile(data, [2.5, 97.5])

accuracy_ci_test = ci95(metrics_test['accuracy'])
precision_ci_test = ci95(metrics_test['precision_weighted'])
recall_ci_test = ci95(metrics_test['recall_weighted'])
f1_ci_test = ci95(metrics_test['f1_weighted'])

print(f"Test Data Accuracy: {np.mean(metrics_test['accuracy'])} (95% CI: {accuracy_ci_test})")
print(f"Test Data Precision (Weighted): {np.mean(metrics_test['precision_weighted'])} (95% CI: {precision_ci_test})")
print(f"Test Data Recall (Weighted): {np.mean(metrics_test['recall_weighted'])} (95% CI: {recall_ci_test})")
print(f"Test Data F1 Score (Weighted): {np.mean(metrics_test['f1_weighted'])} (95% CI: {f1_ci_test})")

conf_matrix_svc = confusion_matrix(y_test, y_pred_test_svc)
accuracy_svc = accuracy_score(y_test, y_pred_test_svc)
report_svc = classification_report(y_test, y_pred_test_svc, zero_division=0)

print("Confusion Matrix (Test Data):\n", conf_matrix_svc)
print("Accuracy:", accuracy_svc)
print("Classification Report (Test Data):\n", report_svc)

kf_svc = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_prob_cv_svc = cross_val_predict(best_svc_model, X_train, y_train, cv=kf_svc, method='predict_proba')[:, 1]
y_pred_cv_svc = cross_val_predict(best_svc_model, X_train, y_train, cv=kf_svc)

accuracy_cv_scores = []
precision_cv_scores = []
recall_cv_scores = []
f1_cv_scores = []

for train_index, test_index in kf_svc.split(X_train):
    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]
    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]
    best_svc_model.fit(X_train_fold, y_train_fold)
    y_pred_fold = best_svc_model.predict(X_test_fold)
    accuracy_cv_scores.append(accuracy_score(y_test_fold, y_pred_fold))
    precision_cv_scores.append(precision_score(y_test_fold, y_pred_fold, average='weighted', zero_division=0))
    recall_cv_scores.append(recall_score(y_test_fold, y_pred_fold, average='weighted'))
    f1_cv_scores.append(f1_score(y_test_fold, y_pred_fold, average='weighted'))

accuracy_ci_cv = ci95(accuracy_cv_scores)
precision_ci_cv = ci95(precision_cv_scores)
recall_ci_cv = ci95(recall_cv_scores)
f1_ci_cv = ci95(f1_cv_scores)

print(f"Cross-Validation Accuracy: {np.mean(accuracy_cv_scores)} (95% CI: {accuracy_ci_cv})")
print(f"Cross-Validation Precision (Weighted): {np.mean(precision_cv_scores)} (95% CI: {precision_ci_cv})")
print(f"Cross-Validation Recall (Weighted): {np.mean(recall_cv_scores)} (95% CI: {recall_ci_cv})")
print(f"Cross-Validation F1 Score (Weighted): {np.mean(f1_cv_scores)} (95% CI: {f1_ci_cv})")

conf_matrix_cv_svc = confusion_matrix(y_train, y_pred_cv_svc)
accuracy_cv_svc = accuracy_score(y_train, y_pred_cv_svc)
report_cv_svc = classification_report(y_train, y_pred_cv_svc, zero_division=0)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv_svc)
print("Cross-Validation Accuracy:", accuracy_cv_svc)
print("Classification Report (Cross-Validation):\n", report_cv_svc)

fpr_svc, tpr_svc, _ = roc_curve(y_test, y_prob_test_svc, pos_label=2)
roc_auc_svc = auc(fpr_svc, tpr_svc)
fpr_cv_svc, tpr_cv_svc, _ = roc_curve(y_train, y_prob_cv_svc, pos_label=2)
roc_auc_cv_svc = auc(fpr_cv_svc, tpr_cv_svc)

def bootstrap_auc(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    auc_scores = []
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_prob = model.predict_proba(X_resampled)[:, 1]
        fpr, tpr, _ = roc_curve(y_resampled, y_prob, pos_label=2)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

fpr_test, tpr_test, _ = roc_curve(y_test, y_prob_test_svc, pos_label=2)
auc_test_sv = auc(fpr_test, tpr_test)
auc_scores_test_sv = bootstrap_auc(best_svc_model, X_test, y_test)

auc_ci_test_sv = np.percentile(auc_scores_test_sv, [2.5, 97.5])

print(f"Test Data AUC: {auc_test_sv} (95% CI: {auc_ci_test_sv})")

def auc_cross_val(model, X, y, cv, pos_label=2):
    auc_scores = []
    for train_index, test_index in cv.split(X):
        model.fit(X[train_index], y[train_index])
        y_prob = model.predict_proba(X[test_index])[:, 1]
        fpr, tpr, _ = roc_curve(y[test_index], y_prob, pos_label=pos_label)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

auc_scores_cv_sv = auc_cross_val(best_svc_model, X_train, y_train, kf_svc, pos_label=2)

def ci95(data):
    return np.percentile(data, [2.5, 97.5])

auc_ci_cv_sv = ci95(auc_scores_cv_sv)

print(f"Cross-Validation AUC: {np.mean(auc_scores_cv_sv)} (95% CI: {auc_ci_cv_sv})")

#Calculation of ROC Curve metrics and graph plotting
fpr_svc, tpr_svc, _ = roc_curve(y_test, y_prob_test_svc, pos_label = 2)
roc_auc_svc = auc(fpr_svc, tpr_svc)

plt.figure(figsize=(8, 8))
plt.plot(fpr_svc, tpr_svc, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_svc))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Test Data')
plt.legend(loc='lower right')
plt.show()
# Plotar a curva ROC para a validação cruzada
fpr_cv_svc, tpr_cv_svc, _ = roc_curve(y_train, y_prob_cv_svc, pos_label = 2)
roc_auc_cv_svc = auc(fpr_cv_svc, tpr_cv_svc)

plt.figure(figsize=(8, 8))
plt.plot(fpr_cv_svc, tpr_cv_svc, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_cv_svc))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Cross-Validation')
plt.legend(loc='lower right')
plt.show()

"""# KNN"""

param_grid = {
    'n_neighbors': [1, 3, 5, 7, 10, 15, 100, 1000],
    'weights': ['uniform', 'distance'],
    'p': [0.001, 0.1, 1, 3, 5, 7, 10, 15, 100, 1000],
    'leaf_size': [0.001, 0.1, 1, 3, 5, 7, 10, 15, 100, 1000]
}

knn_model = KNeighborsClassifier()
grid_search = GridSearchCV(estimator=knn_model, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
best_knn_model = grid_search.best_estimator_
best_knn_model.fit(X_train, y_train)
y_pred_test_knn = best_knn_model.predict(X_test)
y_prob_test_knn = best_knn_model.predict_proba(X_test)[:, 1]

def bootstrap_metrics(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    metrics = {'accuracy': [], 'precision_weighted': [], 'recall_weighted': [], 'f1_weighted': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_pred = model.predict(X_resampled)
        metrics['accuracy'].append(accuracy_score(y_resampled, y_pred))
        metrics['precision_weighted'].append(precision_score(y_resampled, y_pred, average='weighted', zero_division=0))
        metrics['recall_weighted'].append(recall_score(y_resampled, y_pred, average='weighted'))
        metrics['f1_weighted'].append(f1_score(y_resampled, y_pred, average='weighted'))
    return metrics

metrics_test = bootstrap_metrics(best_knn_model, X_test, y_test)

def ci95(data):
    return np.percentile(data, [2.5, 97.5])

accuracy_ci_test = ci95(metrics_test['accuracy'])
precision_ci_test = ci95(metrics_test['precision_weighted'])
recall_ci_test = ci95(metrics_test['recall_weighted'])
f1_ci_test = ci95(metrics_test['f1_weighted'])

print(f"Test Data Accuracy: {np.mean(metrics_test['accuracy'])} (95% CI: {accuracy_ci_test})")
print(f"Test Data Precision (Weighted): {np.mean(metrics_test['precision_weighted'])} (95% CI: {precision_ci_test})")
print(f"Test Data Recall (Weighted): {np.mean(metrics_test['recall_weighted'])} (95% CI: {recall_ci_test})")
print(f"Test Data F1 Score (Weighted): {np.mean(metrics_test['f1_weighted'])} (95% CI: {f1_ci_test})")

conf_matrix_knn = confusion_matrix(y_test, y_pred_test_knn)
accuracy_knn = accuracy_score(y_test, y_pred_test_knn)
report_knn = classification_report(y_test, y_pred_test_knn, zero_division=0)

print("Confusion Matrix (Test Data):\n", conf_matrix_knn)
print("Accuracy:", accuracy_knn)
print("Classification Report (Test Data):\n", report_knn)

kf_knn = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_prob_cv_knn = cross_val_predict(best_knn_model, X_train, y_train, cv=kf_knn, method='predict_proba')[:, 1]
y_pred_cv_knn = cross_val_predict(best_knn_model, X_train, y_train, cv=kf_knn)

accuracy_cv_scores = []
precision_cv_scores = []
recall_cv_scores = []
f1_cv_scores = []

for train_index, test_index in kf_knn.split(X_train):
    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]
    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]
    best_knn_model.fit(X_train_fold, y_train_fold)
    y_pred_fold = best_knn_model.predict(X_test_fold)
    accuracy_cv_scores.append(accuracy_score(y_test_fold, y_pred_fold))
    precision_cv_scores.append(precision_score(y_test_fold, y_pred_fold, average='weighted', zero_division=0))
    recall_cv_scores.append(recall_score(y_test_fold, y_pred_fold, average='weighted'))
    f1_cv_scores.append(f1_score(y_test_fold, y_pred_fold, average='weighted'))

accuracy_ci_cv = ci95(accuracy_cv_scores)
precision_ci_cv = ci95(precision_cv_scores)
recall_ci_cv = ci95(recall_cv_scores)
f1_ci_cv = ci95(f1_cv_scores)

print(f"Cross-Validation Accuracy: {np.mean(accuracy_cv_scores)} (95% CI: {accuracy_ci_cv})")
print(f"Cross-Validation Precision (Weighted): {np.mean(precision_cv_scores)} (95% CI: {precision_ci_cv})")
print(f"Cross-Validation Recall (Weighted): {np.mean(recall_cv_scores)} (95% CI: {recall_ci_cv})")
print(f"Cross-Validation F1 Score (Weighted): {np.mean(f1_cv_scores)} (95% CI: {f1_ci_cv})")

conf_matrix_cv_knn = confusion_matrix(y_train, y_pred_cv_knn)
accuracy_cv_knn = accuracy_score(y_train, y_pred_cv_knn)
report_cv_knn = classification_report(y_train, y_pred_cv_knn, zero_division=0)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv_knn)
print("Cross-Validation Accuracy:", accuracy_cv_knn)
print("Classification Report (Cross-Validation):\n", report_cv_knn)

fpr_knn, tpr_knn, _ = roc_curve(y_test, y_prob_test_knn, pos_label=2)
roc_auc_knn = auc(fpr_knn, tpr_knn)
fpr_cv_knn, tpr_cv_knn, _ = roc_curve(y_train, y_prob_cv_knn, pos_label=2)
roc_auc_cv_knn = auc(fpr_cv_knn, tpr_cv_knn)

def bootstrap_auc(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    auc_scores = []
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_prob = model.predict_proba(X_resampled)[:, 1]
        fpr, tpr, _ = roc_curve(y_resampled, y_prob, pos_label=2)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

fpr_test, tpr_test, _ = roc_curve(y_test, y_prob_test_knn, pos_label=2)
auc_test_knn = auc(fpr_test, tpr_test)
auc_scores_test_knn = bootstrap_auc(best_knn_model, X_test, y_test)

auc_ci_test_knn = np.percentile(auc_scores_test_knn, [2.5, 97.5])

print(f"Test Data AUC: {auc_test_knn} (95% CI: {auc_ci_test_knn})")

def auc_cross_val(model, X, y, cv, pos_label=2):
    auc_scores = []
    for train_index, test_index in cv.split(X):
        model.fit(X[train_index], y[train_index])
        y_prob = model.predict_proba(X[test_index])[:, 1]
        fpr, tpr, _ = roc_curve(y[test_index], y_prob, pos_label=pos_label)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

auc_scores_cv_knn = auc_cross_val(best_knn_model, X_train, y_train, kf_knn, pos_label=2)

def ci95(data):
    return np.percentile(data, [2.5, 97.5])

auc_ci_cv_knn = ci95(auc_scores_cv_knn)

print(f"Cross-Validation AUC: {np.mean(auc_scores_cv_knn)} (95% CI: {auc_ci_cv_knn})")

#Calculation of ROC Curve metrics and graph plotting
fpr_knn, tpr_knn, _ = roc_curve(y_test, y_prob_test_knn, pos_label = 2)
roc_auc_knn = auc(fpr_knn, tpr_knn)

plt.figure(figsize=(8, 8))
plt.plot(fpr_knn, tpr_knn, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_knn))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Test Data')
plt.legend(loc='lower right')
plt.show()

fpr_cv_knn, tpr_cv_knn, _ = roc_curve(y_train, y_prob_cv_knn, pos_label = 2)
roc_auc_cv_knn = auc(fpr_cv_knn, tpr_cv_knn)

plt.figure(figsize=(8, 8))
plt.plot(fpr_cv_knn, tpr_cv_knn, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_cv_knn))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Cross-Validation')
plt.legend(loc='lower right')
plt.show()

"""# MLP CLASSIFIER"""

param_grid = {
    'hidden_layer_sizes': [(10,), (100,), (1000,)],
    'alpha': [0.01, 0.1, 1.0],
    'learning_rate_init': [0.01, 0.1, 1],
    'activation': ['relu', 'logistic', 'tanh'],
    'max_iter': [50, 100, 1000],
    'solver': ['lbfgs', 'sgd', 'adam']
}

mlp_model = MLPClassifier(random_state=RANDOM_STATE)
grid_search = GridSearchCV(estimator=mlp_model, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
best_mlp_model = grid_search.best_estimator_
best_mlp_model.fit(X_train, y_train)
y_pred_test_mlp = best_mlp_model.predict(X_test)
y_prob_test_mlp = best_mlp_model.predict_proba(X_test)[:, 1]

def bootstrap_metrics(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    metrics = {'accuracy': [], 'precision_weighted': [], 'recall_weighted': [], 'f1_weighted': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_pred = model.predict(X_resampled)
        metrics['accuracy'].append(accuracy_score(y_resampled, y_pred))
        metrics['precision_weighted'].append(precision_score(y_resampled, y_pred, average='weighted', zero_division=0))
        metrics['recall_weighted'].append(recall_score(y_resampled, y_pred, average='weighted'))
        metrics['f1_weighted'].append(f1_score(y_resampled, y_pred, average='weighted'))
    return metrics

metrics_test = bootstrap_metrics(best_mlp_model, X_test, y_test)

def ci95(data):
    return np.percentile(data, [2.5, 97.5])

accuracy_ci_test = ci95(metrics_test['accuracy'])
precision_ci_test = ci95(metrics_test['precision_weighted'])
recall_ci_test = ci95(metrics_test['recall_weighted'])
f1_ci_test = ci95(metrics_test['f1_weighted'])

print(f"Test Data Accuracy: {np.mean(metrics_test['accuracy'])} (95% CI: {accuracy_ci_test})")
print(f"Test Data Precision (Weighted): {np.mean(metrics_test['precision_weighted'])} (95% CI: {precision_ci_test})")
print(f"Test Data Recall (Weighted): {np.mean(metrics_test['recall_weighted'])} (95% CI: {recall_ci_test})")
print(f"Test Data F1 Score (Weighted): {np.mean(metrics_test['f1_weighted'])} (95% CI: {f1_ci_test})")

conf_matrix_mlp = confusion_matrix(y_test, y_pred_test_mlp)
accuracy_mlp = accuracy_score(y_test, y_pred_test_mlp)
report_mlp = classification_report(y_test, y_pred_test_mlp, zero_division=0)

print("Confusion Matrix (Test Data):\n", conf_matrix_mlp)
print("Accuracy:", accuracy_mlp)
print("Classification Report (Test Data):\n", report_mlp)

kf_mlp = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_prob_cv_mlp = cross_val_predict(best_mlp_model, X_train, y_train, cv=kf_mlp, method='predict_proba')[:, 1]
y_pred_cv_mlp = cross_val_predict(best_mlp_model, X_train, y_train, cv=kf_mlp)

accuracy_cv_scores = []
precision_cv_scores = []
recall_cv_scores = []
f1_cv_scores = []

for train_index, test_index in kf_mlp.split(X_train):
    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]
    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]
    best_mlp_model.fit(X_train_fold, y_train_fold)
    y_pred_fold = best_mlp_model.predict(X_test_fold)
    accuracy_cv_scores.append(accuracy_score(y_test_fold, y_pred_fold))
    precision_cv_scores.append(precision_score(y_test_fold, y_pred_fold, average='weighted', zero_division=0))
    recall_cv_scores.append(recall_score(y_test_fold, y_pred_fold, average='weighted'))
    f1_cv_scores.append(f1_score(y_test_fold, y_pred_fold, average='weighted'))

accuracy_ci_cv = ci95(accuracy_cv_scores)
precision_ci_cv = ci95(precision_cv_scores)
recall_ci_cv = ci95(recall_cv_scores)
f1_ci_cv = ci95(f1_cv_scores)

print(f"Cross-Validation Accuracy: {np.mean(accuracy_cv_scores)} (95% CI: {accuracy_ci_cv})")
print(f"Cross-Validation Precision (Weighted): {np.mean(precision_cv_scores)} (95% CI: {precision_ci_cv})")
print(f"Cross-Validation Recall (Weighted): {np.mean(recall_cv_scores)} (95% CI: {recall_ci_cv})")
print(f"Cross-Validation F1 Score (Weighted): {np.mean(f1_cv_scores)} (95% CI: {f1_ci_cv})")

conf_matrix_cv_mlp = confusion_matrix(y_train, y_pred_cv_mlp)
accuracy_cv_mlp = accuracy_score(y_train, y_pred_cv_mlp)
report_cv_mlp = classification_report(y_train, y_pred_cv_mlp, zero_division=0)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv_mlp)
print("Cross-Validation Accuracy:", accuracy_cv_mlp)
print("Classification Report (Cross-Validation):\n", report_cv_mlp)

fpr_mlp, tpr_mlp, _ = roc_curve(y_test, y_prob_test_mlp, pos_label=2)
roc_auc_mlp = auc(fpr_mlp, tpr_mlp)
fpr_cv_mlp, tpr_cv_mlp, _ = roc_curve(y_train, y_prob_cv_mlp, pos_label=2)
roc_auc_cv_mlp = auc(fpr_cv_mlp, tpr_cv_mlp)

def bootstrap_auc(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    auc_scores = []
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_prob = model.predict_proba(X_resampled)[:, 1]
        fpr, tpr, _ = roc_curve(y_resampled, y_prob, pos_label=2)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

auc_scores_test_mlp = bootstrap_auc(best_mlp_model, X_test, y_test)
auc_ci_test_mlp = ci95(auc_scores_test_mlp)
print(f"Test Data AUC: {roc_auc_mlp} (95% CI: {auc_ci_test_mlp})")

def auc_cross_val(model, X, y, cv, pos_label=2):
    auc_scores = []
    for train_index, test_index in cv.split(X):
        model.fit(X[train_index], y[train_index])
        y_prob = model.predict_proba(X[test_index])[:, 1]
        fpr, tpr, _ = roc_curve(y[test_index], y_prob, pos_label=2)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

auc_scores_cv_mlp = auc_cross_val(best_mlp_model, X_train, y_train, kf_mlp, pos_label=2)

def ci95(data):
    return np.percentile(data, [2.5, 97.5])

auc_ci_cv_mlp = ci95(auc_scores_cv_mlp)

print(f"Cross-Validation AUC: {np.mean(auc_scores_cv_mlp)} (95% CI: {auc_ci_cv_mlp})")

#Calculation of ROC Curve metrics and graph plotting
fpr_mlp, tpr_mlp, _ = roc_curve(y_test, y_prob_test_mlp, pos_label = 2)
roc_auc_mlp = auc(fpr_mlp, tpr_mlp)
plt.figure(figsize=(8, 8))
plt.plot(fpr_mlp, tpr_mlp, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_mlp))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Test Data')
plt.legend(loc='lower right')
plt.show()

fpr_cv_mlp, tpr_cv_mlp, _ = roc_curve(y_train, y_prob_cv_mlp, pos_label = 2)
roc_auc_cv_mlp = auc(fpr_cv_mlp, tpr_cv_mlp)

plt.figure(figsize=(8, 8))
plt.plot(fpr_cv_mlp, tpr_cv_mlp, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_cv_mlp))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Cross-Validation')
plt.legend(loc='lower right')
plt.show()

"""# DECISION TREE"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV, KFold, cross_val_predict, train_test_split
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             classification_report, confusion_matrix, roc_curve, auc)
from sklearn.utils import resample
import numpy as np

param_grid = {
    'criterion': ['gini', 'entropy'],
    'splitter': ['best', 'random'],
    'max_depth': [None, 5, 10, 15]
}

tree_clf = DecisionTreeClassifier(random_state=RANDOM_STATE)
grid_search = GridSearchCV(estimator=tree_clf, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
best_tree_clf = grid_search.best_estimator_
best_tree_clf.fit(X_train, y_train)
y_pred_test_dt = best_tree_clf.predict(X_test)

def bootstrap_metrics(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    metrics = {'accuracy': [], 'precision_weighted': [], 'recall_weighted': [], 'f1_weighted': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_pred = model.predict(X_resampled)
        metrics['accuracy'].append(accuracy_score(y_resampled, y_pred))
        metrics['precision_weighted'].append(precision_score(y_resampled, y_pred, average='weighted', zero_division=0))
        metrics['recall_weighted'].append(recall_score(y_resampled, y_pred, average='weighted'))
        metrics['f1_weighted'].append(f1_score(y_resampled, y_pred, average='weighted'))
    return metrics

metrics_test = bootstrap_metrics(best_tree_clf, X_test, y_test)

def ci95(data):
    return np.percentile(data, [2.5, 97.5])

accuracy_ci_test = ci95(metrics_test['accuracy'])
precision_ci_test = ci95(metrics_test['precision_weighted'])
recall_ci_test = ci95(metrics_test['recall_weighted'])
f1_ci_test = ci95(metrics_test['f1_weighted'])

print(f"Test Data Accuracy: {np.mean(metrics_test['accuracy'])} (95% CI: {accuracy_ci_test})")
print(f"Test Data Precision (Weighted): {np.mean(metrics_test['precision_weighted'])} (95% CI: {precision_ci_test})")
print(f"Test Data Recall (Weighted): {np.mean(metrics_test['recall_weighted'])} (95% CI: {recall_ci_test})")
print(f"Test Data F1 Score (Weighted): {np.mean(metrics_test['f1_weighted'])} (95% CI: {f1_ci_test})")

conf_matrix_dt = confusion_matrix(y_test, y_pred_test_dt)
accuracy_dt = accuracy_score(y_test, y_pred_test_dt)
report_dt = classification_report(y_test, y_pred_test_dt, zero_division=0)

print("Confusion Matrix (Test Data):\n", conf_matrix_dt)
print("Accuracy:", accuracy_dt)
print("Classification Report (Test Data):\n", report_dt)

kf_dt = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_pred_cv_dt = cross_val_predict(best_tree_clf, X_train, y_train, cv=kf_dt)

accuracy_cv_scores = []
precision_cv_scores = []
recall_cv_scores = []
f1_cv_scores = []

for train_index, test_index in kf_dt.split(X_train):
    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]
    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]
    best_tree_clf.fit(X_train_fold, y_train_fold)
    y_pred_fold = best_tree_clf.predict(X_test_fold)
    accuracy_cv_scores.append(accuracy_score(y_test_fold, y_pred_fold))
    precision_cv_scores.append(precision_score(y_test_fold, y_pred_fold, average='weighted', zero_division=0))
    recall_cv_scores.append(recall_score(y_test_fold, y_pred_fold, average='weighted'))
    f1_cv_scores.append(f1_score(y_test_fold, y_pred_fold, average='weighted'))

accuracy_ci_cv = ci95(accuracy_cv_scores)
precision_ci_cv = ci95(precision_cv_scores)
recall_ci_cv = ci95(recall_cv_scores)
f1_ci_cv = ci95(f1_cv_scores)

print(f"Cross-Validation Accuracy: {np.mean(accuracy_cv_scores)} (95% CI: {accuracy_ci_cv})")
print(f"Cross-Validation Precision (Weighted): {np.mean(precision_cv_scores)} (95% CI: {precision_ci_cv})")
print(f"Cross-Validation Recall (Weighted): {np.mean(recall_cv_scores)} (95% CI: {recall_ci_cv})")
print(f"Cross-Validation F1 Score (Weighted): {np.mean(f1_cv_scores)} (95% CI: {f1_ci_cv})")

conf_matrix_cv_dt = confusion_matrix(y_train, y_pred_cv_dt)
accuracy_cv_dt = accuracy_score(y_train, y_pred_cv_dt)
report_cv_dt = classification_report(y_train, y_pred_cv_dt, zero_division=0)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv_dt)
print("Cross-Validation Accuracy:", accuracy_cv_dt)
print("Classification Report (Cross-Validation):\n", report_cv_dt)

y_prob_test_dt = best_tree_clf.predict_proba(X_test)[:, 1]
y_prob_cv_dt = cross_val_predict(best_tree_clf, X_train, y_train, cv=kf_dt, method='predict_proba')[:, 1]

fpr_dt, tpr_dt, _ = roc_curve(y_test, y_prob_test_dt, pos_label=2)
roc_auc_dt = auc(fpr_dt, tpr_dt)
fpr_cv_dt, tpr_cv_dt, _ = roc_curve(y_train, y_prob_cv_dt, pos_label=2)
roc_auc_cv_dt = auc(fpr_cv_dt, tpr_cv_dt)

def bootstrap_auc(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    auc_scores = []
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_prob = model.predict_proba(X_resampled)[:, 1]
        fpr, tpr, _ = roc_curve(y_resampled, y_prob, pos_label=2)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

auc_scores_test_dt = bootstrap_auc(best_tree_clf, X_test, y_test)
auc_ci_test_dt = np.percentile(auc_scores_test_dt, [2.5, 97.5])

print(f"Test Data AUC: {roc_auc_dt} (95% CI: {auc_ci_test_dt})")

def auc_cross_val(model, X, y, cv, pos_label=2):
    auc_scores = []
    for train_index, test_index in cv.split(X):
        model.fit(X[train_index], y[train_index])
        y_prob = model.predict_proba(X[test_index])[:, 1]
        fpr, tpr, _ = roc_curve(y[test_index], y_prob, pos_label=pos_label)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

auc_scores_cv_dt = auc_cross_val(best_tree_clf, X_train, y_train, kf_dt, pos_label=2)

auc_ci_cv_dt = ci95(auc_scores_cv_dt)

print(f"Cross-Validation AUC: {np.mean(auc_scores_cv_dt)} (95% CI: {auc_ci_cv_dt})")

#Calculation of ROC Curve metrics and graph plotting

from sklearn.model_selection import StratifiedKFold
def plot_roc_curve(fpr, tpr, auc_score, title='Receiver Operating Characteristic'):
    plt.figure(figsize=(8, 8))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc_score))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(title)
    plt.legend(loc='lower right')
    plt.show()
y_prob_test_dt = best_tree_clf.predict_proba(X_test)[:, 1]
fpr_dt, tpr_dt, _ = roc_curve(y_test, y_prob_test_dt, pos_label = 2)
roc_auc_dt = auc(fpr_dt, tpr_dt)
plot_roc_curve(fpr_dt, tpr_dt, roc_auc_dt, title='ROC - Decision Tree - Test Data')
kf_dt = StratifiedKFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_prob_cv_dt = cross_val_predict(best_tree_clf, X_train, y_train, cv=kf_dt, method='predict_proba')[:, 1]
fpr_cv_dt, tpr_cv_dt, _ = roc_curve(y_train, y_prob_cv_dt, pos_label = 2)
roc_auc_cv_dt = auc(fpr_cv_dt, tpr_cv_dt)
plot_roc_curve(fpr_cv_dt, tpr_cv_dt, roc_auc_cv_dt, title='ROC - Decision Tree - Cross-Validation')

#FEATURE IMPORTANCE DECISION TREE
X_2 = []
X_2
features_2 =[]
imp_features_dt2 = []
df_imp_features_dt2 = []
X_2 = df.drop('sex', axis=1)
features_2 = []

for column in X_2.columns:
    features_2.append(column)

imp_features_dt2 = best_tree_clf.feature_importances_
df_imp_features_dt2 = pd.DataFrame({"features": features_2, "weights": imp_features_dt2})
df_imp_features_dt2_sorted = df_imp_features_dt2.sort_values(by='weights', ascending=True)
plt.figure(figsize=(10, 6))
colors = plt.cm.cividis(np.linspace(0.2, 1, len(df_imp_features_dt2_sorted)))
bars = plt.barh(df_imp_features_dt2_sorted['features'], df_imp_features_dt2_sorted['weights'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')
for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()

plt.show()

"""# RANDOM FOREST"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, KFold, cross_val_predict
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             classification_report, confusion_matrix, roc_curve, auc)
from sklearn.utils import resample
import numpy as np

param_grid = {
    'n_estimators': [5, 50, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 10, 15],
    'min_samples_leaf': [1, 4, 6],
    'max_features': ['auto', 'sqrt'],
    'criterion': ['gini', 'entropy']
}

rf_model = RandomForestClassifier(random_state=RANDOM_STATE)
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)
best_rf_model = grid_search.best_estimator_
best_rf_model.fit(X_train, y_train)
y_pred_test_rf = best_rf_model.predict(X_test)

def bootstrap_metrics(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    metrics = {'accuracy': [], 'precision_weighted': [], 'recall_weighted': [], 'f1_weighted': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_pred = model.predict(X_resampled)
        metrics['accuracy'].append(accuracy_score(y_resampled, y_pred))
        metrics['precision_weighted'].append(precision_score(y_resampled, y_pred, average='weighted', zero_division=0))
        metrics['recall_weighted'].append(recall_score(y_resampled, y_pred, average='weighted'))
        metrics['f1_weighted'].append(f1_score(y_resampled, y_pred, average='weighted'))
    return metrics

metrics_test = bootstrap_metrics(best_rf_model, X_test, y_test)

def ci95(data):
    return np.percentile(data, [2.5, 97.5])

accuracy_ci_test = ci95(metrics_test['accuracy'])
precision_ci_test = ci95(metrics_test['precision_weighted'])
recall_ci_test = ci95(metrics_test['recall_weighted'])
f1_ci_test = ci95(metrics_test['f1_weighted'])

print(f"Test Data Accuracy: {np.mean(metrics_test['accuracy'])} (95% CI: {accuracy_ci_test})")
print(f"Test Data Precision (Weighted): {np.mean(metrics_test['precision_weighted'])} (95% CI: {precision_ci_test})")
print(f"Test Data Recall (Weighted): {np.mean(metrics_test['recall_weighted'])} (95% CI: {recall_ci_test})")
print(f"Test Data F1 Score (Weighted): {np.mean(metrics_test['f1_weighted'])} (95% CI: {f1_ci_test})")

conf_matrix_rf = confusion_matrix(y_test, y_pred_test_rf)
accuracy_rf = accuracy_score(y_test, y_pred_test_rf)
report_rf = classification_report(y_test, y_pred_test_rf, zero_division=0)

print("Confusion Matrix (Test Data):\n", conf_matrix_rf)
print("Accuracy:", accuracy_rf)
print("Classification Report (Test Data):\n", report_rf)

kf_rf = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_pred_cv_rf = cross_val_predict(best_rf_model, X_train, y_train, cv=kf_rf)

accuracy_cv_scores = []
precision_cv_scores = []
recall_cv_scores = []
f1_cv_scores = []

for train_index, test_index in kf_rf.split(X_train):
    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]
    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]
    best_rf_model.fit(X_train_fold, y_train_fold)
    y_pred_fold = best_rf_model.predict(X_test_fold)
    accuracy_cv_scores.append(accuracy_score(y_test_fold, y_pred_fold))
    precision_cv_scores.append(precision_score(y_test_fold, y_pred_fold, average='weighted', zero_division=0))
    recall_cv_scores.append(recall_score(y_test_fold, y_pred_fold, average='weighted'))
    f1_cv_scores.append(f1_score(y_test_fold, y_pred_fold, average='weighted'))

accuracy_ci_cv = ci95(accuracy_cv_scores)
precision_ci_cv = ci95(precision_cv_scores)
recall_ci_cv = ci95(recall_cv_scores)
f1_ci_cv = ci95(f1_cv_scores)

print(f"Cross-Validation Accuracy: {np.mean(accuracy_cv_scores)} (95% CI: {accuracy_ci_cv})")
print(f"Cross-Validation Precision (Weighted): {np.mean(precision_cv_scores)} (95% CI: {precision_ci_cv})")
print(f"Cross-Validation Recall (Weighted): {np.mean(recall_cv_scores)} (95% CI: {recall_ci_cv})")
print(f"Cross-Validation F1 Score (Weighted): {np.mean(f1_cv_scores)} (95% CI: {f1_ci_cv})")

conf_matrix_cv_rf = confusion_matrix(y_train, y_pred_cv_rf)
accuracy_cv_rf = accuracy_score(y_train, y_pred_cv_rf)
report_cv_rf = classification_report(y_train, y_pred_cv_rf, zero_division=0)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv_rf)
print("Cross-Validation Accuracy:", accuracy_cv_rf)
print("Classification Report (Cross-Validation):\n", report_cv_rf)

y_prob_test_rf = best_rf_model.predict_proba(X_test)[:, 1]
y_prob_cv_rf = cross_val_predict(best_rf_model, X_train, y_train, cv=kf_rf, method='predict_proba')[:, 1]

fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_test_rf, pos_label=2)
roc_auc_rf = auc(fpr_rf, tpr_rf)
fpr_cv_rf, tpr_cv_rf, _ = roc_curve(y_train, y_prob_cv_rf, pos_label=2)
roc_auc_cv_rf = auc(fpr_cv_rf, tpr_cv_rf)

def bootstrap_auc(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    auc_scores = []
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_prob = model.predict_proba(X_resampled)[:, 1]
        fpr, tpr, _ = roc_curve(y_resampled, y_prob, pos_label=2)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

auc_scores_test_rf = bootstrap_auc(best_rf_model, X_test, y_test)
auc_ci_test_rf = np.percentile(auc_scores_test_rf, [2.5, 97.5])

print(f"Test Data AUC: {roc_auc_rf} (95% CI: {auc_ci_test_rf})")

def auc_cross_val(model, X, y, cv, pos_label=2):
    auc_scores = []
    for train_index, test_index in cv.split(X):
        model.fit(X[train_index], y[train_index])
        y_prob = model.predict_proba(X[test_index])[:, 1]
        fpr, tpr, _ = roc_curve(y[test_index], y_prob, pos_label=pos_label)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

auc_scores_cv_rf = auc_cross_val(best_rf_model, X_train, y_train, kf_rf, pos_label=2)

auc_ci_cv_rf = ci95(auc_scores_cv_rf)

print(f"Cross-Validation AUC: {np.mean(auc_scores_cv_rf)} (95% CI: {auc_ci_cv_rf})")

#Calculation of ROC Curve metrics and graph plotting

fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_test_rf, pos_label = 2)
roc_auc_rf = auc(fpr_rf, tpr_rf)

plt.figure(figsize=(8, 8))
plt.plot(fpr_rf, tpr_rf, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_rf))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Test Data')
plt.legend(loc='lower right')
plt.show()
fpr_cv_rf, tpr_cv_rf, _ = roc_curve(y_train, y_prob_cv_rf, pos_label = 2)
roc_auc_cv_rf = auc(fpr_cv_rf, tpr_cv_rf)

plt.figure(figsize=(8, 8))
plt.plot(fpr_cv_rf, tpr_cv_rf, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_cv_rf))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Cross-Validation')
plt.legend(loc='lower right')
plt.show()

#FEATURE IMPORTANCE RANDOM FOREST CLASSIFIER

X_3 = df.drop('sex', axis=1)
features_3 = []

for column in X_3.columns:
    features_3.append(column)

imp_features_dt3 = best_rf_model.feature_importances_
df_imp_features_dt3 = pd.DataFrame({"features": features_3, "weights": imp_features_dt3})
df_imp_features_dt3_sorted = df_imp_features_dt3.sort_values(by='weights', ascending=True)
plt.figure(figsize=(10, 6))
colors = plt.cm.inferno(np.linspace(0.2, 1, len(df_imp_features_dt3_sorted)))
bars = plt.barh(df_imp_features_dt3_sorted['features'], df_imp_features_dt3_sorted['weights'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')
for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()

plt.show()

"""#**Adaboosting**"""

import numpy as np
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV, KFold, cross_val_predict
from sklearn.metrics import (confusion_matrix, accuracy_score, classification_report,
                             precision_score, recall_score, f1_score, roc_curve, auc)
from sklearn.utils import resample

param_grid = {
    'n_estimators': [5, 50, 200],
    'learning_rate': [0.01, 0.1, 1.0],
    'estimator__max_depth': [1, 2, 3],  # Usando DecisionTreeClassifier como base
    'estimator__min_samples_split': [2, 10, 15],
    'estimator__min_samples_leaf': [1, 4, 6]
}

base_clf = DecisionTreeClassifier(random_state=RANDOM_STATE)

ada_clf = AdaBoostClassifier(estimator=base_clf, random_state=RANDOM_STATE)

grid_search = GridSearchCV(estimator=ada_clf, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

best_ada_clf = grid_search.best_estimator_
best_ada_clf.fit(X_train, y_train)

y_pred_test_ada = best_ada_clf.predict(X_test)
y_prob_test_ada = best_ada_clf.predict_proba(X_test)[:, 1]

def bootstrap_metrics(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    metrics = {'accuracy': [], 'precision_weighted': [], 'recall_weighted': [], 'f1_weighted': []}
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_pred = model.predict(X_resampled)
        metrics['accuracy'].append(accuracy_score(y_resampled, y_pred))
        metrics['precision_weighted'].append(precision_score(y_resampled, y_pred, average='weighted', zero_division=0))
        metrics['recall_weighted'].append(recall_score(y_resampled, y_pred, average='weighted'))
        metrics['f1_weighted'].append(f1_score(y_resampled, y_pred, average='weighted'))
    return metrics

metrics_test = bootstrap_metrics(best_ada_clf, X_test, y_test)

def ci95(data):
    return np.percentile(data, [2.5, 97.5])

accuracy_ci_test = ci95(metrics_test['accuracy'])
precision_ci_test = ci95(metrics_test['precision_weighted'])
recall_ci_test = ci95(metrics_test['recall_weighted'])
f1_ci_test = ci95(metrics_test['f1_weighted'])

print(f"Test Data Accuracy: {np.mean(metrics_test['accuracy'])} (95% CI: {accuracy_ci_test})")
print(f"Test Data Precision (Weighted): {np.mean(metrics_test['precision_weighted'])} (95% CI: {precision_ci_test})")
print(f"Test Data Recall (Weighted): {np.mean(metrics_test['recall_weighted'])} (95% CI: {recall_ci_test})")
print(f"Test Data F1 Score (Weighted): {np.mean(metrics_test['f1_weighted'])} (95% CI: {f1_ci_test})")

conf_matrix = confusion_matrix(y_test, y_pred_test_ada)
accuracy_ada = accuracy_score(y_test, y_pred_test_ada)
report = classification_report(y_test, y_pred_test_ada, zero_division=0)

print("Confusion Matrix (Test Data):\n", conf_matrix)
print("Accuracy:", accuracy_ada)
print("Classification Report (Test Data):\n", report)

kf_ada = KFold(n_splits=fold, shuffle=True, random_state=RANDOM_STATE)
y_pred_cv_ada = cross_val_predict(best_ada_clf, X_train, y_train, cv=kf_ada)
y_prob_cv_ada = cross_val_predict(best_ada_clf, X_train, y_train, cv=kf_ada, method='predict_proba')[:, 1]

accuracy_cv_scores = []
precision_cv_scores = []
recall_cv_scores = []
f1_cv_scores = []

for train_index, test_index in kf_ada.split(X_train):
    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]
    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]
    best_ada_clf.fit(X_train_fold, y_train_fold)
    y_pred_fold = best_ada_clf.predict(X_test_fold)
    accuracy_cv_scores.append(accuracy_score(y_test_fold, y_pred_fold))
    precision_cv_scores.append(precision_score(y_test_fold, y_pred_fold, average='weighted', zero_division=0))
    recall_cv_scores.append(recall_score(y_test_fold, y_pred_fold, average='weighted'))
    f1_cv_scores.append(f1_score(y_test_fold, y_pred_fold, average='weighted'))

accuracy_ci_cv = ci95(accuracy_cv_scores)
precision_ci_cv = ci95(precision_cv_scores)
recall_ci_cv = ci95(recall_cv_scores)
f1_ci_cv = ci95(f1_cv_scores)

print(f"Cross-Validation Accuracy: {np.mean(accuracy_cv_scores)} (95% CI: {accuracy_ci_cv})")
print(f"Cross-Validation Precision (Weighted): {np.mean(precision_cv_scores)} (95% CI: {precision_ci_cv})")
print(f"Cross-Validation Recall (Weighted): {np.mean(recall_cv_scores)} (95% CI: {recall_ci_cv})")
print(f"Cross-Validation F1 Score (Weighted): {np.mean(f1_cv_scores)} (95% CI: {f1_ci_cv})")

conf_matrix_cv = confusion_matrix(y_train, y_pred_cv_ada)
accuracy_cv_ada = accuracy_score(y_train, y_pred_cv_ada)
report_cv = classification_report(y_train, y_pred_cv_ada, zero_division=0)

print("Confusion Matrix (Cross-Validation):\n", conf_matrix_cv)
print("Cross-Validation Accuracy:", accuracy_cv_ada)
print("Classification Report (Cross-Validation):\n", report_cv)

fpr_test, tpr_test, _ = roc_curve(y_test, y_prob_test_ada, pos_label=2)
roc_auc_test_ada = auc(fpr_test, tpr_test)
fpr_cv, tpr_cv, _ = roc_curve(y_train, y_prob_cv_ada, pos_label=2)
roc_auc_cv = auc(fpr_cv, tpr_cv)

def bootstrap_auc(model, X, y, n_iterations=1000, random_state=RANDOM_STATE):
    np.random.seed(random_state)
    auc_scores = []
    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        y_prob = model.predict_proba(X_resampled)[:, 1]
        fpr, tpr, _ = roc_curve(y_resampled, y_prob, pos_label=2)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

auc_scores_test_ada = bootstrap_auc(best_ada_clf, X_test, y_test)
auc_ci_test_ada = np.percentile(auc_scores_test_ada, [2.5, 97.5])

print(f"Test Data AUC: {roc_auc_test_ada} (95% CI: {auc_ci_test_ada})")

def auc_cross_val(model, X, y, cv, pos_label=2):
    auc_scores = []
    for train_index, test_index in cv.split(X):
        model.fit(X[train_index], y[train_index])
        y_prob = model.predict_proba(X[test_index])[:, 1]
        fpr, tpr, _ = roc_curve(y[test_index], y_prob, pos_label=pos_label)
        auc_scores.append(auc(fpr, tpr))
    return auc_scores

auc_scores_cv_ada = auc_cross_val(best_ada_clf, X_train, y_train, kf_ada)

auc_ci_cv_ada = np.percentile(auc_scores_cv_ada, [2.5, 97.5])

print(f"Cross-Validation AUC: {np.mean(auc_scores_cv_ada)} (95% CI: {auc_ci_cv_ada})")

fpr_ada, tpr_ada, _ = roc_curve(y_test, y_prob_test_ada, pos_label = 2)
roc_auc_ada = auc(fpr_ada, tpr_ada)

plt.figure(figsize=(8, 8))
plt.plot(fpr_ada, tpr_ada, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_ada))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Test Data')
plt.legend(loc='lower right')
plt.show()
fpr_cv_ada, tpr_cv_ada, _ = roc_curve(y_train, y_prob_cv_ada, pos_label = 2)
roc_auc_cv_ada = auc(fpr_cv_ada, tpr_cv_ada)

plt.figure(figsize=(8, 8))
plt.plot(fpr_cv_ada, tpr_cv_ada, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_cv_ada))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic - Cross-Validation')
plt.legend(loc='lower right')
plt.show()

#FEATURE IMPORTANCE - ADABOOSTING

X_3 = df.drop('sex', axis=1)
features_3 = X_3.columns.tolist()
imp_features_ada = best_ada_clf.feature_importances_

df_imp_features_ada = pd.DataFrame({"features": features_3, "weights": imp_features_ada})
df_imp_features_ada_sorted = df_imp_features_ada.sort_values(by='weights', ascending=True)

plt.figure(figsize=(10, 6))
colors = plt.cm.inferno(np.linspace(0.2, 1, len(df_imp_features_ada_sorted)))
bars = plt.barh(df_imp_features_ada_sorted['features'], df_imp_features_ada_sorted['weights'], color=colors)
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance')

for bar in bars:
    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, round(bar.get_width(), 4),
             va='center', ha='left', fontsize=10, color='white')

plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.gca().set_facecolor('white')
plt.tight_layout()

plt.show()

"""# Plot-ROC"""

plt.style.use('seaborn-whitegrid')
fig, axs = plt.subplots(1, 2, figsize=(20, 8))  # Criando subplots com 1 linha e 2 colunas

axs[0].plot(fpr_knn, tpr_knn, color='blue', lw=2, label='K Nearest Neighbors (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_knn, auc_ci_test_knn[0], auc_ci_test_knn[1]))
axs[0].plot(fpr_svc, tpr_svc, color='green', lw=2, label='Support Vector Machine (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_svc, auc_ci_test_sv[0], auc_ci_test_sv[1]))
axs[0].plot(fpr_logreg, tpr_logreg, color='red', lw=2, label='Logistic Regression (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_logreg, auc_ci_test_lr[0], auc_ci_test_lr[1]))
axs[0].plot(fpr_mlp, tpr_mlp, color='purple', lw=2, label='Multilayer Perceptron (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_mlp, auc_ci_test_mlp[0], auc_ci_test_mlp[1]))
axs[0].plot(fpr_gb, tpr_gb, color='orange', lw=2, label='Gradient Boosting (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_gb, auc_ci_test_gb[0], auc_ci_test_gb[1]))
axs[0].plot(fpr_ada, tpr_ada, color='red', lw=2, label='Adaboost Classifier (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_ada, auc_ci_test_ada[0], auc_ci_test_ada[1]))
axs[0].plot(fpr_rf, tpr_rf, color='deeppink', lw=2, label='Random Forest (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_rf, auc_ci_test_rf[0], auc_ci_test_rf[1]))
axs[0].plot(fpr_logreg, tpr_logreg, color='red', lw=2, label='Decision Tree (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_dt, auc_ci_test_dt[0], auc_ci_test_dt[1]))


axs[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
axs[0].set_xlim([0.0, 1.0])
axs[0].set_ylim([0.0, 1.05])
axs[0].set_xlabel('False Positive Rate')
axs[0].set_ylabel('True Positive Rate')
axs[0].set_title('Receiver Operating Characteristic (ROC) Curve')
axs[0].legend(loc="lower right")



axs[1].plot(fpr_cv_svc, tpr_cv_svc, color='green', lw=2, label='Support Vector Machine (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_cv_svc, auc_ci_cv_sv[0], auc_ci_cv_sv[1]))
axs[1].plot(fpr_cv_logreg, tpr_cv_logreg, color='red', lw=2, label='Logistic Regression (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_cv_logreg, auc_ci_cv_lr[0], auc_ci_cv_lr[1]))
axs[1].plot(fpr_cv_mlp, tpr_cv_mlp, color='purple', lw=2, label='Multilayer Perceptron (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_cv_mlp, auc_ci_cv_mlp[0], auc_ci_cv_mlp[1]))
axs[1].plot(fpr_cv_knn, tpr_cv_knn, color='blue', lw=2, label='K Nearest Neighbors (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_cv_knn, auc_ci_cv_knn[0], auc_ci_cv_knn[1]))
axs[1].plot(fpr_cv_ada, tpr_cv_ada, color='orange', lw=2, label='Adaboost Classifier (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_cv_ada, auc_ci_cv_ada[0], auc_ci_cv_ada[1]))
axs[1].plot(fpr_cv_rf, tpr_cv_rf, color='deeppink', lw=2, label='Random Forest (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_cv_rf, auc_ci_cv_rf[0], auc_ci_cv_rf[1]))
axs[1].plot(fpr_cv_gb, tpr_cv_gb, color='orange', lw=2, label='Gradient Boosting (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_cv_gb, auc_ci_cv_gb[0], auc_ci_cv_gb[1]))
axs[1].plot(fpr_cv_dt, tpr_cv_dt, color='gray', lw=2, label='Decision Tree (AUC = %0.2f, CI 95%%: [%0.2f, %0.2f])' % (roc_auc_cv_dt, auc_ci_cv_dt[0], auc_ci_cv_dt[1]))

axs[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
axs[1].set_xlim([0.0, 1.0])
axs[1].set_ylim([0.0, 1.05])
axs[1].set_xlabel('False Positive Rate')
axs[1].set_ylabel('True Positive Rate')
axs[1].set_title('Receiver Operating Characteristic (ROC) Curve')
axs[1].legend(loc="lower right")
axs[0].set_title('Receiver Operating Characteristic - Test Data')
axs[1].set_title('Receiver Operating Characteristic - Cross-Validation (5-Folds)')
plt.savefig('ROCCURVE.jpg', dpi=600, bbox_inches='tight')
plt.show()

###FEATURE IMPORTANCE

X = df.drop('sex', axis=1)
features = X.columns

colors = plt.cm.magma(np.linspace(0.2, 0.8, 5))  # Adicionei uma cor extra para evitar repetição

imp_features_gb = best_gb_model.feature_importances_
imp_features_lr = np.abs(best_logreg_model.coef_[0])
imp_features_dt = best_tree_clf.feature_importances_
imp_features_rf = best_rf_model.feature_importances_
imp_features_ada = best_ada_clf.feature_importances_  # Adicionando importância das features do AdaBoost

sorted_indices_gb = np.argsort(imp_features_gb)
sorted_indices_lr = np.argsort(imp_features_lr)
sorted_indices_dt = np.argsort(imp_features_dt)
sorted_indices_rf = np.argsort(imp_features_rf)
sorted_indices_ada = np.argsort(imp_features_ada)  # Ordenando para AdaBoost

fig, axs = plt.subplots(2, 3, figsize=(15, 10))
fig.suptitle('Feature Importance', fontsize=16)

# PLOT 1: Gradient Boosting
axs[0, 0].barh(features[sorted_indices_gb], imp_features_gb[sorted_indices_gb], color=colors[0])
axs[0, 0].set_title('Gradient Boosting')
axs[0, 0].set_xlabel('Importance')
axs[0, 0].set_ylabel('Feature')
axs[0, 0].grid(axis='x', linestyle='--', alpha=0.7)
axs[0, 0].set_facecolor('#f0f0f0')  # Cinza claro

# PLOT 2: Logistic Regression
axs[0, 1].barh(features[sorted_indices_lr], imp_features_lr[sorted_indices_lr], color=colors[1])
axs[0, 1].set_title('Logistic Regression')
axs[0, 1].set_xlabel('Importance')
axs[0, 1].set_ylabel('Feature')
axs[0, 1].grid(axis='x', linestyle='--', alpha=0.7)
axs[0, 1].set_facecolor('#f0f0f0')  # Cinza claro

# PLOT 3: Decision Tree
axs[0, 2].barh(features[sorted_indices_dt], imp_features_dt[sorted_indices_dt], color=colors[2])
axs[0, 2].set_title('Decision Tree')
axs[0, 2].set_xlabel('Importance')
axs[0, 2].set_ylabel('Feature')
axs[0, 2].grid(axis='x', linestyle='--', alpha=0.7)
axs[0, 2].set_facecolor('#f0f0f0')  # Cinza claro

# PLOT 4: Random Forest
axs[1, 0].barh(features[sorted_indices_rf], imp_features_rf[sorted_indices_rf], color=colors[3])
axs[1, 0].set_title('Random Forest')
axs[1, 0].set_xlabel('Importance')
axs[1, 0].set_ylabel('Feature')
axs[1, 0].grid(axis='x', linestyle='--', alpha=0.7)
axs[1, 0].set_facecolor('#f0f0f0')  # Cinza claro

# PLOT 5: Adaboosting
axs[1, 1].barh(features[sorted_indices_ada], imp_features_ada[sorted_indices_ada], color=colors[4])  # Usando a cor extra
axs[1, 1].set_title('AdaBoost')
axs[1, 1].set_xlabel('Importance')
axs[1, 1].set_ylabel('Feature')
axs[1, 1].grid(axis='x', linestyle='--', alpha=0.7)
axs[1, 1].set_facecolor('#f0f0f0')  # Cinza claro

fig.delaxes(axs[1, 2])

plt.tight_layout(rect=[0, 0, 1, 0.96], h_pad=1.5)

plt.savefig('FEATUREIMPORTANCE.jpg', dpi=300, bbox_inches='tight')
plt.show()

import numpy as np
from sklearn.metrics import roc_auc_score
from sklearn.utils import resample

y_prob_test_gb = y_prob_test_gb
y_prob_test_lr = y_prob_test_logreg
y_prob_test_knn = y_prob_test_knn
y_prob_test_ada = y_prob_test_ada
y_prob_test_svc = y_prob_test_svc
y_prob_test_mlp = y_prob_test_mlp
y_prob_test_dt = y_prob_test_dt
y_prob_test_rf = y_prob_test_rf

roc_auc_gb = roc_auc_gb
roc_auc_knn = roc_auc_knn
roc_auc_ada = roc_auc_ada
roc_auc_svc = roc_auc_svc
roc_auc_mlp = roc_auc_mlp
roc_auc_lr = roc_auc_logreg
roc_auc_dt = roc_auc_dt
roc_auc_rf = roc_auc_rf

def compare_auc_bootstrap(y_true, y_prob1, y_prob2, n_iterations=1000, random_state=24):
    np.random.seed(random_state)
    auc_diffs = []
    for _ in range(n_iterations):
        # Resample with replacement
        indices = np.random.choice(len(y_true), len(y_true), replace=True)
        y_true_resampled = y_true[indices]
        y_prob1_resampled = y_prob1[indices]
        y_prob2_resampled = y_prob2[indices]

        # Calculate AUC for both models
        auc1 = roc_auc_score(y_true_resampled, y_prob1_resampled)
        auc2 = roc_auc_score(y_true_resampled, y_prob2_resampled)
        auc_diffs.append(auc1 - auc2)

    mean_diff = np.mean(auc_diffs)
    ci_diff = np.percentile(auc_diffs, [2.5, 97.5])

    return mean_diff, ci_diff

model_names = ["gb", "knn", "ada", "svc", "mlp", "lr", "dt", "rf"]
auc_scores = {
    "gb": roc_auc_gb,
    "knn": roc_auc_knn,
    "ada": roc_auc_ada,
    "svc": roc_auc_svc,
    "mlp": roc_auc_mlp,
    "lr": roc_auc_lr,
    "dt": roc_auc_dt,
    "rf": roc_auc_rf
}
y_probs = {
    "gb": y_prob_test_gb,
    "knn": y_prob_test_knn,
    "ada": y_prob_test_ada,
    "svc": y_prob_test_svc,
    "mlp": y_prob_test_mlp,
    "lr": y_prob_test_lr,
    "dt": y_prob_test_dt,
    "rf": y_prob_test_rf
}

for i, model1 in enumerate(model_names):
    for model2 in model_names[i+1:]:
        mean_diff, ci_diff = compare_auc_bootstrap(np.array(y_test), np.array(y_probs[model1]), np.array(y_probs[model2]))
        print(f"Difference in AUC between {model1} and {model2}: {mean_diff:.3f} (95% CI: {ci_diff})")

import matplotlib.pyplot as plt
import numpy as np

model_pairs = [("gb", "knn"), ("gb", "ada"), ("gb", "svc"),
               ("gb", "mlp"), ("gb", "lr"), ("gb", "dt"),
               ("gb", "rf"),
               ("knn", "ada"), ("knn", "svc"), ("knn", "mlp"),
               ("knn", "lr"), ("knn", "dt"), ("knn", "rf"),
               ("ada", "svc"), ("ada", "mlp"), ("ada", "lr"),
               ("ada", "dt"), ("ada", "rf"),
               ("svc", "mlp"), ("svc", "lr"), ("svc", "dt"),
               ("svc", "rf"),
               ("mlp", "lr"), ("mlp", "dt"), ("mlp", "rf"),
               ("lr", "dt"), ("lr", "rf"),
               ("dt", "rf")]

mean_diffs = []
cis = []

for model1, model2 in model_pairs:
    mean_diff, ci_diff = compare_auc_bootstrap(np.array(y_test), np.array(y_probs[model1]), np.array(y_probs[model2]))
    mean_diffs.append(mean_diff)
    cis.append(ci_diff)

lower_bounds = np.array([ci[0] for ci in cis])
upper_bounds = np.array([ci[1] for ci in cis])

model_labels = [f"{model_pair[0].upper()} vs {model_pair[1].upper()}" for model_pair in model_pairs]

colors = ['b' if mean_diff >= 0 else 'r' for mean_diff in mean_diffs]  # Azul se positivo, vermelho se negativo
bar_colors = ['lightblue' if mean_diff >= 0 else 'lightcoral' for mean_diff in mean_diffs]  # Cores das barras
edge_colors = ['black' if mean_diff >= 0 else 'darkred' for mean_diff in mean_diffs]  # Cores das bordas das barras

plt.figure(figsize=(16, 10))
bars = plt.bar(model_labels, mean_diffs, yerr=[np.abs(lower_bounds - np.array(mean_diffs)), np.abs(upper_bounds - np.array(mean_diffs))], capsize=8, color=bar_colors, edgecolor=edge_colors)
plt.axhline(y=0, color='black', linestyle='--', linewidth=1.5)  # Linha horizontal em y=0 para referência
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.yticks(fontsize=12)
plt.xlabel('Model Pairs', fontsize=14)
plt.ylabel('Difference in AUC', fontsize=14)
plt.title('Difference in AUC between Model Pairs with 95% CI', fontsize=16)
plt.grid(axis='y', linestyle='--', alpha=0.7)

legend_labels = ['Positive Difference', 'Negative Difference']
legend_handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) for color in ['lightblue', 'lightcoral']]
plt.legend(legend_handles, legend_labels, loc='upper left', fontsize=12)

legend_text = {
    "GB": "Gradient Boosting",
    "KNN": "K-Nearest Neighbors",
    "ADA": "AdaBoost",
    "SVC": "Support Vector Classifier",
    "MLP": "Multi-layer Perceptron",
    "LR": "Logistic Regression",
    "DT": "Decision Tree",
    "RF": "Random Forest"
}

plt.text(0.02, 0.02, "\n".join(f"{key}: {value}" for key, value in legend_text.items()), transform=plt.gca().transAxes, fontsize=12, verticalalignment='bottom', bbox=dict(facecolor='white', alpha=0.5))

plt.tight_layout()
plt.savefig('Meandifference.jpg', dpi=300, bbox_inches='tight')
plt.show()